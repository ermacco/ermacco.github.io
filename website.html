<!DOCTYPE html>
<html lang="it">
<head>
  <meta charset="UTF-8">
  <title>EchoSynch</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    :root {
      --primary-color: #0078d7;
      --light-color: #f5f5f5;
      --dark-color: #222;
      --accent-color: #e6f0fa;
      --text-color: #333;
      --header-height: 60px;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: var(--light-color);
      color: var(--text-color);
      line-height: 1.6;
    }

    header {
      background: var(--primary-color);
      color: #fff;
      padding: 4rem 1rem 2rem 1rem;
      text-align: center;
      background-image: url("https://www.publicdomainpictures.net/pictures/660000/velka/kommunikation-1734155741LZf.jpg");
      background-size: cover;
      background-position: center;
      position: relative;
    }

    header::after {
      content: "";
      position: absolute;
      top: 0; left: 0;
      width: 100%; height: 100%;
      background: rgba(0, 0, 0, 0.4);
      z-index: 1;
    }

    header h1 {
      position: relative;
      z-index: 2;
      font-size: 2.5rem;
      text-shadow: 0 2px 4px rgba(0,0,0,0.6);
    }

    #lang-btn {
      position: absolute;
      top: 20px;
      right: 20px;
      z-index: 3;
      background: #fff;
      color: var(--primary-color);
      border: none;
      padding: 0.5rem 1rem;
      border-radius: 20px;
      font-weight: bold;
      cursor: pointer;
      box-shadow: 0 2px 8px rgba(0,0,0,0.08);
      transition: background 0.2s;
      font-size: 1.2rem;
    }
    #lang-btn:hover {
      background: var(--accent-color);
    }

    main {
      max-width: 900px;
      margin: 2rem auto;
      background: #fff;
      padding: 2rem;
      border-radius: 10px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.08);
    }

    section {
      margin-bottom: 2rem;
    }

    section h2 {
      color: var(--primary-color);
      margin-bottom: 1rem;
      border-bottom: 2px solid var(--accent-color);
      padding-bottom: 0.5rem;
    }

    section h3 {
      margin-top: 2rem;
      color: #222;
    }

    ul, ol {
      margin-left: 1.5rem;
      margin-bottom: 1rem;
    }

    pre {
      background: #f5f5f5;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      font-size: 1rem;
      margin-bottom: 1rem;
    }

    .contact {
      background: var(--accent-color);
      padding: 1.5rem;
      border-radius: 8px;
    }

    footer {
      text-align: center;
      padding: 1.5rem;
      background: var(--dark-color);
      color: #fff;
      margin-top: 2rem;
    }

    @media (max-width: 600px) {
      header h1 {
        font-size: 1.8rem;
      }
      main {
        padding: 1.5rem;
      }
      #lang-btn {
        top: 10px;
        right: 10px;
        padding: 0.3rem 0.7rem;
      }
    }
  </style>
</head>
<body>

  <header>
    <h1 id="main-title">Benvenuti ad EchoSynch</h1>
    <button id="lang-btn">üá¨üáß</button>
  </header>

  <main>
    <section id="presentazione">
      <h2 id="about-title">üåç EchoSynch: Comunicazione Inclusiva Rivoluzionata</h2>
      <p id="about-status"><strong>Stato:</strong> In sviluppo ‚Ä¢ <strong>Lingua:</strong> Italiano</p>

      <h3 id="toc-title">Indice</h3>
      <ul id="toc-list">
        <li>Introduzione</li>
        <li>Come funziona</li>
        <li>Tecnologie utilizzate</li>
        <li>Applicazioni pratiche</li>
        <li>Sfide e limiti attuali</li>
        <li>Requisiti</li>
        <li>Installazione</li>
        <li>Utilizzo</li>
        <li>Struttura repository</li>
        <li>Contributi</li>
        <li>Roadmap</li>
        <li>Impatto sociale e conclusione</li>
      </ul>

      <h3 id="intro-title">Introduzione</h3>
      <p id="about-text">
        Immagina un mondo dove le parole non sono solo suoni, ma si trasformano in gesti visibili. Un mondo dove la comunicazione √® davvero accessibile a tutti, senza barriere.<br><br>
        EchoSynch nasce proprio con questo scopo: costruire un ponte tra persone sorde e udenti grazie alla tecnologia. Attraverso un avatar 3D realistico, il nostro sistema traduce l‚Äôaudio parlato in LIS (Lingua dei Segni Italiana) in tempo reale.<br><br>
        Ogni gesto √® progettato con precisione per rispettare la grammatica e la struttura della LIS. Questo permette a milioni di persone di accedere immediatamente a video, trasmissioni TV, conferenze online e conversazioni quotidiane.
      </p>

      <h3 id="how-title">Come funziona</h3>
      <ol id="how-list">
        <li><strong>Riconoscimento vocale</strong> ‚Äì L‚Äôaudio in ingresso viene analizzato e trascritto in tempo reale dal sistema speech-to-text.</li>
        <li><strong>Traduzione LIS</strong> ‚Äì Il testo viene interpretato da un motore linguistico che mappa le frasi ai segni LIS corrispondenti.</li>
        <li><strong>Animazione 3D</strong> ‚Äì L‚Äôavatar esegue i gesti con movimenti del corpo ed espressioni, fondamentali per la corretta comprensione della lingua dei segni.</li>
      </ol>
      <p id="how-result">üëâ Il risultato √® una comunicazione immediata, naturale e accessibile.</p>

      <h3 id="tech-title">Tecnologie utilizzate</h3>
      <ul id="tech-list">
        <li><strong>THREE.js</strong> ‚Üí per modellazione e animazione 3D direttamente sul web, con movimenti fluidi e realistici.</li>
        <li><strong>Blender</strong> ‚Üí per creare il modello 3D e definire le animazioni LIS.</li>
        <li><strong>HTML e integrazione web</strong> ‚Üí per un‚Äôinterfaccia semplice, compatibile e accessibile su ogni dispositivo.</li>
        <li><strong>Motori di riconoscimento vocale</strong> ‚Üí per convertire l‚Äôaudio in testo in tempo reale, base per la traduzione LIS.</li>
      </ul>
      <p id="tech-note">Ogni componente √® stata scelta per garantire affidabilit√†, realismo e facilit√† d‚Äôuso.</p>

      <h3 id="app-title">Applicazioni pratiche</h3>
      <p id="app-text">Stiamo lavorando per integrare EchoSynch nelle principali piattaforme di streaming come YouTube, Netflix, Prime Video, Disney+ e altre, per rendere i contenuti pi√π accessibili alla comunit√† sorda.</p>
      <ul id="app-list">
        <li>üì∫ <strong>Trasmissioni TV</strong> ‚Üí rendere i programmi live pi√π accessibili.</li>
        <li>üéì <strong>Educazione online</strong> ‚Üí garantire un‚Äôesperienza di apprendimento davvero inclusiva per studenti sordi.</li>
        <li>üé§ <strong>Eventi e conferenze</strong> ‚Üí offrire traduzione simultanea senza bisogno costante di interpreti.</li>
      </ul>
      <p id="app-note">üëâ Attualmente, nulla di simile esiste sul mercato.</p>

      <h3 id="ch-title">Sfide e limiti attuali</h3>
      <ul id="ch-list">
        <li><strong>Compatibilit√† browser</strong> ‚Äì Funziona bene su Chrome, ma restano problemi su Edge, Firefox e Opera.</li>
        <li><strong>Qualit√† dei movimenti</strong> ‚Äì Alcuni gesti dell‚Äôavatar non sono ancora abbastanza fluidi.</li>
        <li><strong>Vocabolario LIS incompleto</strong> ‚Äì L‚Äôavatar 3D attuale non copre ancora tutta la gamma dei segni LIS di base.</li>
      </ul>
      <p id="ch-note">Queste sfide fanno parte del nostro lavoro in corso: superarle √® essenziale per offrire uno strumento davvero affidabile e inclusivo.</p>

      <h3 id="req-title">Requisiti</h3>
      <ul id="req-list">
        <li>Node.js &gt;= 14</li>
        <li>Python 3.8+</li>
        <li>Blender installato (per modifiche ai modelli)</li>
        <li>Browser compatibile WebGL</li>
      </ul>

      <h3 id="inst-title">Installazione</h3>
      <pre id="inst-text">
# clona il repository
git clone https://github.com/&lt;your-username&gt;/echosynch.git
cd echosynch

# installa le dipendenze
npm install

# avvia in modalit√† sviluppo
npm run dev
      </pre>

      <h3 id="use-title">Utilizzo</h3>
      <pre id="use-text">
# esempio avvio locale
npm run start
Oppure, come libreria:

import { EchoSynch } from "echosynch";

EchoSynch.start({
  audioInput: "mic",
  avatar: "default",
});
      </pre>

      <h3 id="repo-title">Struttura repository</h3>
      <pre id="repo-text">
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ src/                # codice sorgente
‚îú‚îÄ‚îÄ models/             # avatar 3D e animazioni
‚îú‚îÄ‚îÄ tests/              # test automatici
‚îú‚îÄ‚îÄ docs/               # documentazione
‚îú‚îÄ‚îÄ requirements.txt    # dipendenze Python (se presenti)
‚îú‚îÄ‚îÄ package.json        # dipendenze Node
‚îî‚îÄ‚îÄ .github/            # workflow CI / template issue
      </pre>

      <h3 id="cont-title">Contributi</h3>
      <ul id="cont-list">
        <li>Forka il repository</li>
        <li>Crea un branch con nome descrittivo (feat/nome-funzionalit√†)</li>
        <li>Apri una Pull Request descrivendo le modifiche</li>
        <li>Segui gli standard di codice e assicurati che i test siano superati</li>
      </ul>

      <h3 id="road-title">Roadmap</h3>
      <ul id="road-list">
        <li>Integrazione YouTube</li>
        <li>Supporto a pi√π lingue dei segni</li>
        <li>Miglioramento espressioni facciali avatar</li>
        <li>Estensione per TV e piattaforme streaming</li>
      </ul>

      <h3 id="soc-title">Impatto sociale e conclusione</h3>
      <p id="soc-text">EchoSynch non √® solo un progetto tecnologico: √® un‚Äôiniziativa con forte impatto sociale. Oggi molte persone sorde si affidano ai sottotitoli per comprendere i contenuti digitali. Ma i sottotitoli non sempre colgono le sfumature linguistiche e non rappresentano la LIS, che √® una lingua vera e propria con grammatica propria.</p>
      <p id="soc-note">Con EchoSynch la comunicazione diventa pi√π immediata, naturale e coinvolgente. Significa:</p>
      <ul id="soc-list">
        <li>‚úÖ pi√π autonomia,</li>
        <li>‚úÖ pi√π inclusione,</li>
        <li>‚úÖ pi√π opportunit√† di apprendimento, intrattenimento e partecipazione sociale.</li>
      </ul>
      <p id="soc-end">Il nostro obiettivo √® costruire un mondo pi√π empatico, dove udenti e sordi possano comunicare senza barriere. La tecnologia parla con le mani. E finalmente, tutti possono ascoltare.</p>
    </section>
  </main>

  <footer id="footer-text">
    &copy; 2025 EchoSynch. Tutti i diritti riservati.
  </footer>

  <script>
    const translations = {
      it: {
        mainTitle: "Benvenuti ad EchoSynch",
        aboutTitle: "üåç EchoSynch: Comunicazione Inclusiva Rivoluzionata",
        aboutStatus: "<strong>Stato:</strong> In sviluppo ‚Ä¢ <strong>Lingua:</strong> Italiano",
        tocTitle: "Indice",
        tocList: [
          "Introduzione",
          "Come funziona",
          "Tecnologie utilizzate",
          "Applicazioni pratiche",
          "Sfide e limiti attuali",
          "Requisiti",
          "Installazione",
          "Utilizzo",
          "Struttura repository",
          "Contributi",
          "Roadmap",
          "Impatto sociale e conclusione"
        ],
        introTitle: "Introduzione",
        aboutText: `Immagina un mondo dove le parole non sono solo suoni, ma si trasformano in gesti visibili. Un mondo dove la comunicazione √® davvero accessibile a tutti, senza barriere.<br><br>
EchoSynch nasce proprio con questo scopo: costruire un ponte tra persone sorde e udenti grazie alla tecnologia. Attraverso un avatar 3D realistico, il nostro sistema traduce l‚Äôaudio parlato in LIS (Lingua dei Segni Italiana) in tempo reale.<br><br>
Ogni gesto √® progettato con precisione per rispettare la grammatica e la struttura della LIS. Questo permette a milioni di persone di accedere immediatamente a video, trasmissioni TV, conferenze online e conversazioni quotidiane.`,
        howTitle: "Come funziona",
        howList: [
          "<strong>Riconoscimento vocale</strong> ‚Äì L‚Äôaudio in ingresso viene analizzato e trascritto in tempo reale dal sistema speech-to-text.",
          "<strong>Traduzione LIS</strong> ‚Äì Il testo viene interpretato da un motore linguistico che mappa le frasi ai segni LIS corrispondenti.",
          "<strong>Animazione 3D</strong> ‚Äì L‚Äôavatar esegue i gesti con movimenti del corpo ed espressioni, fondamentali per la corretta comprensione della lingua dei segni."
        ],
        howResult: "üëâ Il risultato √® una comunicazione immediata, naturale e accessibile.",
        techTitle: "Tecnologie utilizzate",
        techList: [
          "<strong>THREE.js</strong> ‚Üí per modellazione e animazione 3D direttamente sul web, con movimenti fluidi e realistici.",
          "<strong>Blender</strong> ‚Üí per creare il modello 3D e definire le animazioni LIS.",
          "<strong>HTML e integrazione web</strong> ‚Üí per un‚Äôinterfaccia semplice, compatibile e accessibile su ogni dispositivo.",
          "<strong>Motori di riconoscimento vocale</strong> ‚Üí per convertire l‚Äôaudio in testo in tempo reale, base per la traduzione LIS."
        ],
        techNote: "Ogni componente √® stata scelta per garantire affidabilit√†, realismo e facilit√† d‚Äôuso.",
        appTitle: "Applicazioni pratiche",
        appText: "Stiamo lavorando per integrare EchoSynch nelle principali piattaforme di streaming come YouTube, Netflix, Prime Video, Disney+ e altre, per rendere i contenuti pi√π accessibili alla comunit√† sorda.",
        appList: [
          "üì∫ <strong>Trasmissioni TV</strong> ‚Üí rendere i programmi live pi√π accessibili.",
          "üéì <strong>Educazione online</strong> ‚Üí garantire un‚Äôesperienza di apprendimento davvero inclusiva per studenti sordi.",
          "üé§ <strong>Eventi e conferenze</strong> ‚Üí offrire traduzione simultanea senza bisogno costante di interpreti."
        ],
        appNote: "üëâ Attualmente, nulla di simile esiste sul mercato.",
        chTitle: "Sfide e limiti attuali",
        chList: [
          "<strong>Compatibilit√† browser</strong> ‚Äì Funziona bene su Chrome, ma restano problemi su Edge, Firefox e Opera.",
          "<strong>Qualit√† dei movimenti</strong> ‚Äì Alcuni gesti dell‚Äôavatar non sono ancora abbastanza fluidi.",
          "<strong>Vocabolario LIS incompleto</strong> ‚Äì L‚Äôavatar 3D attuale non copre ancora tutta la gamma dei segni LIS di base."
        ],
        chNote: "Queste sfide fanno parte del nostro lavoro in corso: superarle √® essenziale per offrire uno strumento davvero affidabile e inclusivo.",
        reqTitle: "Requisiti",
        reqList: [
          "Node.js &gt;= 14",
          "Python 3.8+",
          "Blender installato (per modifiche ai modelli)",
          "Browser compatibile WebGL"
        ],
        instTitle: "Installazione",
        instText: `# clona il repository
git clone https://github.com/&lt;your-username&gt;/echosynch.git
cd echosynch

# installa le dipendenze
npm install

# avvia in modalit√† sviluppo
npm run dev`,
        useTitle: "Utilizzo",
        useText: `# esempio avvio locale
npm run start
Oppure, come libreria:

import { EchoSynch } from "echosynch";

EchoSynch.start({
  audioInput: "mic",
  avatar: "default",
});`,
        repoTitle: "Struttura repository",
        repoText: `‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ src/                # codice sorgente
‚îú‚îÄ‚îÄ models/             # avatar 3D e animazioni
‚îú‚îÄ‚îÄ tests/              # test automatici
‚îú‚îÄ‚îÄ docs/               # documentazione
‚îú‚îÄ‚îÄ requirements.txt    # dipendenze Python (se presenti)
‚îú‚îÄ‚îÄ package.json        # dipendenze Node
‚îî‚îÄ‚îÄ .github/            # workflow CI / template issue`,
        contTitle: "Contributi",
        contList: [
          "Forka il repository",
          "Crea un branch con nome descrittivo (feat/nome-funzionalit√†)",
          "Apri una Pull Request descrivendo le modifiche",
          "Segui gli standard di codice e assicurati che i test siano superati"
        ],
        roadTitle: "Roadmap",
        roadList: [
          "Integrazione YouTube",
          "Supporto a pi√π lingue dei segni",
          "Miglioramento espressioni facciali avatar",
          "Estensione per TV e piattaforme streaming"
        ],
        socTitle: "Impatto sociale e conclusione",
        socText: "EchoSynch non √® solo un progetto tecnologico: √® un‚Äôiniziativa con forte impatto sociale. Oggi molte persone sorde si affidano ai sottotitoli per comprendere i contenuti digitali. Ma i sottotitoli non sempre colgono le sfumature linguistiche e non rappresentano la LIS, che √® una lingua vera e propria con grammatica propria.",
        socNote: "Con EchoSynch la comunicazione diventa pi√π immediata, naturale e coinvolgente. Significa:",
        socList: [
          "‚úÖ pi√π autonomia,",
          "‚úÖ pi√π inclusione,",
          "‚úÖ pi√π opportunit√† di apprendimento, intrattenimento e partecipazione sociale."
        ],
        socEnd: "Il nostro obiettivo √® costruire un mondo pi√π empatico, dove udenti e sordi possano comunicare senza barriere. La tecnologia parla con le mani. E finalmente, tutti possono ascoltare.",
        footerText: "&copy; 2025 EchoSynch. Tutti i diritti riservati."
      },
      en: {
        mainTitle: "Welcome to EchoSynch",
        aboutTitle: "üåç EchoSynch: Inclusive Communication Revolutionized",
        aboutStatus: "<strong>Status:</strong> WIP ‚Ä¢ <strong>Language:</strong> English",
        tocTitle: "Table of Contents",
        tocList: [
          "Introduction",
          "How It Works",
          "Technologies Used",
          "Practical Applications",
          "Challenges and Current Limitations",
          "Requirements",
          "Installation",
          "Usage",
          "Repository Structure",
          "Contributing",
          "Roadmap",
          "Social Impact and Conclusion"
        ],
        introTitle: "Introduction",
        aboutText: `Imagine a world where words are not just sounds, but transform into visible gestures. A world where communication is truly accessible to everyone, without barriers.<br><br>
EchoSynch was created with this very purpose: to build a bridge between deaf and hearing people through technology. Through a realistic 3D avatar, our system translates spoken audio into LIS (Italian Sign Language) in real time.<br><br>
Each gesture is precisely designed to respect the grammar and structure of LIS. This allows millions of people to immediately access videos, TV broadcasts, online conferences, and everyday conversations.`,
        howTitle: "How It Works",
        howList: [
          "<strong>Speech Recognition</strong> ‚Äì The input audio is analyzed and transcribed in real time by the speech-to-text system.",
          "<strong>LIS Translation</strong> ‚Äì The text is interpreted by a linguistic engine that maps sentences to corresponding LIS signs.",
          "<strong>3D Animation</strong> ‚Äì The avatar performs gestures with body movements and expressions, which are essential for correct comprehension of sign language."
        ],
        howResult: "üëâ The result is immediate, natural, and accessible communication.",
        techTitle: "Technologies Used",
        techList: [
          "<strong>THREE.js</strong> ‚Üí for 3D modeling and animation directly on the web, with smooth and realistic movements.",
          "<strong>Blender</strong> ‚Üí to create the 3D model and define LIS-related animations.",
          "<strong>HTML and web integration</strong> ‚Üí for a simple, compatible, and accessible interface on any device.",
          "<strong>Speech recognition engines</strong> ‚Üí to convert audio into text in real time, serving as the basis for LIS translation."
        ],
        techNote: "Each component was chosen to ensure reliability, realism, and ease of use.",
        appTitle: "Practical Applications",
        appText: "We are also working to integrate EchoSynch into major streaming platforms such as YouTube, Netflix, Prime Video, Disney+, and others, to make their content more accessible to the deaf community.",
        appList: [
          "üì∫ <strong>TV broadcasts</strong> ‚Üí making live programs more accessible.",
          "üéì <strong>Online education</strong> ‚Üí ensuring a truly inclusive learning experience for deaf students.",
          "üé§ <strong>Events and conferences</strong> ‚Üí offering simultaneous translation without the constant need for interpreters."
        ],
        appNote: "üëâ Currently, nothing like this exists on the market.",
        chTitle: "Challenges and Current Limitations",
        chList: [
          "<strong>Browser compatibility</strong> ‚Äì Works smoothly on Chrome, but issues remain with Edge, Firefox, and Opera.",
          "<strong>Dynamic motion quality</strong> ‚Äì Some avatar gestures are not fluid enough.",
          "<strong>Incomplete LIS vocabulary</strong> ‚Äì The current 3D avatar does not yet cover the full range of basic LIS signs."
        ],
        chNote: "These challenges are part of our ongoing work in progress, and overcoming them is essential to deliver a fully reliable and inclusive tool.",
        reqTitle: "Requirements",
        reqList: [
          "Node.js &gt;= 14",
          "Python 3.8+",
          "Blender installed (for model editing)",
          "WebGL-compatible browser"
        ],
        instTitle: "Installation",
        instText: `# clone the repository
git clone https://github.com/&lt;your-username&gt;/echosynch.git
cd echosynch

# install dependencies
npm install

# start in development mode
npm run dev`,
        useTitle: "Usage",
        useText: `# example local run
npm run start
Or, if used as a library:

import { EchoSynch } from "echosynch";

EchoSynch.start({
  audioInput: "mic",
  avatar: "default",
});`,
        repoTitle: "Repository Structure",
        repoText: `‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ src/                # source code
‚îú‚îÄ‚îÄ models/             # 3D avatars and animations
‚îú‚îÄ‚îÄ tests/              # automated tests
‚îú‚îÄ‚îÄ docs/               # documentation
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies (if any)
‚îú‚îÄ‚îÄ package.json        # Node dependencies
‚îî‚îÄ‚îÄ .github/            # CI workflows / issue templates`,
        contTitle: "Contributing",
        contList: [
          "Fork the repository",
          "Create a branch with a descriptive name (feat/feature-name)",
          "Open a Pull Request describing your changes",
          "Follow code standards and ensure tests pass"
        ],
        roadTitle: "Roadmap",
        roadList: [
          "YouTube integration",
          "Support for multiple sign languages",
          "Improved avatar facial expressions",
          "Extension for TV and streaming platforms"
        ],
        socTitle: "Social Impact and Conclusion",
        socText: "EchoSynch is not just a technological project: it‚Äôs an initiative with a strong social impact. Today, many deaf people rely on subtitles to understand digital content. But subtitles don‚Äôt always capture linguistic nuances and don‚Äôt represent LIS, which is a full-fledged language with its own grammar.",
        socNote: "With EchoSynch, communication becomes more immediate, natural, and engaging. It means:",
        socList: [
          "‚úÖ more autonomy,",
          "‚úÖ more inclusion,",
          "‚úÖ more opportunities for learning, entertainment, and social participation."
        ],
        socEnd: "Our goal is to build a more empathetic world, where hearing and deaf people can communicate without barriers. Technology speaks with hands. And finally, everyone can listen.",
        footerText: "&copy; 2025 EchoSynch. All rights reserved."
      }
    };

    let currentLang = "it";
    document.getElementById("lang-btn").onclick = function() {
      currentLang = currentLang === "it" ? "en" : "it";
      this.textContent = currentLang === "it" ? "üá¨üáß" : "üáÆüáπ";
      const t = translations[currentLang];
      document.getElementById("main-title").textContent = t.mainTitle;
      document.getElementById("about-title").innerHTML = t.aboutTitle;
      document.getElementById("about-status").innerHTML = t.aboutStatus;
      document.getElementById("toc-title").textContent = t.tocTitle;
      document.getElementById("toc-list").innerHTML = t.tocList.map(item => `<li>${item}</li>`).join("");
      document.getElementById("intro-title").textContent = t.introTitle;
      document.getElementById("about-text").innerHTML = t.aboutText;
      document.getElementById("how-title").textContent = t.howTitle;
      document.getElementById("how-list").innerHTML = t.howList.map(item => `<li>${item}</li>`).join("");
      document.getElementById("how-result").textContent = t.howResult;
      document.getElementById("tech-title").textContent = t.techTitle;
      document.getElementById("tech-list").innerHTML = t.techList.map(item => `<li>${item}</li>`).join("");
      document.getElementById("tech-note").textContent = t.techNote;
      document.getElementById("app-title").textContent = t.appTitle;
      document.getElementById("app-text").textContent = t.appText;
      document.getElementById("app-list").innerHTML = t.appList.map(item => `<li>${item}</li>`).join("");
      document.getElementById("app-note").textContent = t.appNote;
      document.getElementById("ch-title").textContent = t.chTitle;
      document.getElementById("ch-list").innerHTML = t.chList.map(item => `<li>${item}</li>`).join("");
      document.getElementById("ch-note").textContent = t.chNote;
      document.getElementById("req-title").textContent = t.reqTitle;
      document.getElementById("req-list").innerHTML = t.reqList.map(item => `<li>${item}</li>`).join("");
      document.getElementById("inst-title").textContent = t.instTitle;
      document.getElementById("inst-text").textContent = t.instText;
      document.getElementById("use-title").textContent = t.useTitle;
      document.getElementById("use-text").textContent = t.useText;
      document.getElementById("repo-title").textContent = t.repoTitle;
      document.getElementById("repo-text").textContent = t.repoText;
      document.getElementById("cont-title").textContent = t.contTitle;
      document.getElementById("cont-list").innerHTML = t.contList.map(item => `<li>${item}</li>`).join("");
      document.getElementById("road-title").textContent = t.roadTitle;
      document.getElementById("road-list").innerHTML = t.roadList.map(item => `<li>${item}</li>`).join("");
      document.getElementById("soc-title").textContent = t.socTitle;
      document.getElementById("soc-text").textContent = t.socText;
      document.getElementById("soc-note").textContent = t.socNote;
      document.getElementById("soc-list").innerHTML = t.socList.map(item => `<li>${item}</li>`).join("");
      document.getElementById("soc-end").textContent = t.socEnd;
      document.getElementById("footer-text").innerHTML = t.footerText;
    };
  </script>
</body>
</html>