<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>EchoSynch</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    :root {
      --primary-color: #0078d7;
      --light-color: #f5f5f5;
      --dark-color: #222;
      --accent-color: #e6f0fa;
      --text-color: #333;
      --header-height: 60px;
    }
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: var(--light-color);
      color: var(--text-color);
      line-height: 1.6;
    }
    header {
      background: var(--primary-color);
      color: #fff;
      padding: 4rem 1rem 2rem 1rem;
      text-align: center;
      background-image: url("https://www.publicdomainpictures.net/pictures/660000/velka/kommunikation-1734155741LZf.jpg");
      background-size: cover;
      background-position: center;
      position: relative;
    }
    header::after {
      content: "";
      position: absolute;
      top: 0; left: 0;
      width: 100%; height: 100%;
      background: rgba(0, 0, 0, 0.4);
      z-index: 1;
    }
    header h1 {
      position: relative;
      z-index: 2;
      font-size: 2.5rem;
      text-shadow: 0 2px 4px rgba(0,0,0,0.6);
    }
    main {
      max-width: 900px;
      margin: 2rem auto;
      background: #fff;
      padding: 2rem;
      border-radius: 10px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.08);
    }
    section { margin-bottom: 2rem; }
    section h2 {
      color: var(--primary-color);
      margin-bottom: 1rem;
      border-bottom: 2px solid var(--accent-color);
      padding-bottom: 0.5rem;
    }
    section h3 {
      margin-top: 2rem;
      color: #222;
    }
    ul, ol { margin-left: 1.5rem; margin-bottom: 1rem; }
    pre {
      background: #f5f5f5;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      font-size: 1rem;
      margin-bottom: 1rem;
    }
    .contact {
      background: var(--accent-color);
      padding: 1.5rem;
      border-radius: 8px;
    }
    footer {
      text-align: center;
      padding: 1.5rem;
      background: var(--dark-color);
      color: #fff;
      margin-top: 2rem;
    }
    @media (max-width: 600px) {
      header h1 { font-size: 1.8rem; }
      main { padding: 1.5rem; }
    }
  </style>
</head>
<body>

  <header>
    <h1>üåç EchoSynch: Inclusive Communication Revolutionized</h1>
    <p><strong>Status:</strong> WIP ‚Ä¢ <strong>Language:</strong> English</p>
  </header>

  <main>
    <section>
      <h2>Table of Contents</h2>
      <ul>
        <li>Introduction</li>
        <li>How It Works</li>
        <li>Technologies Used</li>
        <li>Practical Applications</li>
        <li>Animations Completed</li>
        <li>Challenges and Current Limitations</li>
        <li>Requirements</li>
        <li>Installation</li>
        <li>Usage</li>
        <li>Repository Structure</li>
        <li>Contributing</li>
        <li>Roadmap</li>
        <li>Social Impact and Conclusion</li>
      </ul>

      <h3>Introduction</h3>
      <p>
        Imagine a world where words are not just sounds, but transform into visible gestures. A world where communication is truly accessible to everyone, without barriers.<br><br>
        EchoSynch was created with this very purpose: to build a bridge between deaf and hearing people through technology. Through a realistic 3D avatar, our system translates spoken audio into LIS (Italian Sign Language) in real time.<br><br>
        Each gesture is precisely designed to respect the grammar and structure of LIS. This allows millions of people to immediately access videos, TV broadcasts, online conferences, and everyday conversations.
      </p>

      <h3>How It Works</h3>
      <p>How does EchoSynch work? The process happens in three main stages:</p>
      <ol>
        <li><strong>Speech Recognition</strong> ‚Äì The input audio is analyzed and transcribed in real time by the speech-to-text system.</li>
        <li><strong>LIS Translation</strong> ‚Äì The text is interpreted by a linguistic engine that maps sentences to corresponding LIS signs.</li>
        <li><strong>3D Animation</strong> ‚Äì The avatar performs gestures with body movements and expressions, which are essential for correct comprehension of sign language.</li>
      </ol>
      <p>üëâ The result is immediate, natural, and accessible communication.</p>

      <h3>Technologies Used</h3>
      <ul>
        <li><strong>THREE.js</strong> ‚Üí for 3D modeling and animation directly on the web, with smooth and realistic movements.</li>
        <li><strong>Blender</strong> ‚Üí to create the 3D model and define LIS-related animations.</li>
        <li><strong>HTML and web integration</strong> ‚Üí for a simple, compatible, and accessible interface on any device.</li>
        <li><strong>Speech recognition engines</strong> ‚Üí to convert audio into text in real time, serving as the basis for LIS translation.</li>
      </ul>
      <p>Each component was chosen to ensure reliability, realism, and ease of use.</p>

      <h3>Practical Applications</h3>
      <p>We are also working to integrate EchoSynch into major streaming platforms such as YouTube, Netflix, Prime Video, Disney+, and others, to make their content more accessible to the deaf community.</p>
      <p>But applications don‚Äôt stop there:</p>
      <ul>
        <li>üì∫ <strong>TV broadcasts</strong> ‚Üí making live programs more accessible.</li>
        <li>üéì <strong>Online education</strong> ‚Üí ensuring a truly inclusive learning experience for deaf students.</li>
        <li>üé§ <strong>Events and conferences</strong> ‚Üí offering simultaneous translation without the constant need for interpreters.</li>
      </ul>
      <p>üëâ Currently, nothing like this exists on the market.</p>

      <h3>Animations Completed</h3>
      <ul>
        <li>Ciao</li>
        <li>Come stai?</li>
        <li>Io</li>
        <li>Tu</li>
        <li>Bene</li>
        <li>Aiutare</li>
      </ul>

      <h3>Challenges and Current Limitations</h3>
      <ul>
        <li><strong>Browser compatibility</strong> ‚Äì Works smoothly on Chrome, but issues remain with Edge, Firefox, and Opera.</li>
        <li><strong>Dynamic motion quality</strong> ‚Äì Some avatar gestures are not fluid enough.</li>
        <li><strong>Incomplete LIS vocabulary</strong> ‚Äì The current 3D avatar does not yet cover the full range of basic LIS signs.</li>
      </ul>
      <p>These challenges are part of our ongoing work in progress, and overcoming them is essential to deliver a fully reliable and inclusive tool.</p>

      <h3>Requirements</h3>
      <ul>
        <li>Node.js &gt;= 14</li>
        <li>Python 3.8+</li>
        <li>Blender installed (for model editing)</li>
        <li>WebGL-compatible browser</li>
      </ul>

      <h3>Installation</h3>
      <pre>
# clone the repository
git clone https://github.com/ermacco/EchoSynch.git
cd echosynch

# install dependencies
npm install

# start in development mode
npm run dev
      </pre>

      <h3>Usage</h3>
      <pre>
# example local run
npm run start
Or, if used as a library:

import { EchoSynch } from "echosynch";

EchoSynch.start({
  audioInput: "mic",
  avatar: "default",
});
      </pre>

      <h3>Repository Structure</h3>
      <pre>
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ main/
  ‚îú‚îÄ‚îÄ src/
  ‚îú‚îÄ‚îÄ website/               # source code
  ‚îú‚îÄ‚îÄ package.json        # Node dependencies
‚îî‚îÄ‚îÄ .github/            # CI workflows / issue templates
      </pre>
      <p><a href="https://github.com/ermacco/EchoSynch" target="_blank">View repository on GitHub</a></p>

      <h3>Contributing</h3>
      <ul>
        <li>Fork the repository</li>
        <li>Create a branch with a descriptive name (feat/feature-name)</li>
        <li>Open a Pull Request describing your changes</li>
        <li>Follow code standards and ensure tests pass</li>
      </ul>

      <h3>Roadmap</h3>
      <ul>
        <li>YouTube integration</li>
        <li>Support for multiple sign languages</li>
        <li>Improved avatar facial expressions</li>
        <li>Extension for TV and streaming platforms</li>
      </ul>

      <h3>Social Impact and Conclusion</h3>
      <p>EchoSynch is not just a technological project: it‚Äôs an initiative with a strong social impact. Today, many deaf people rely on subtitles to understand digital content. But subtitles don‚Äôt always capture linguistic nuances and don‚Äôt represent LIS, which is a full-fledged language with its own grammar.</p>
      <p>With EchoSynch, communication becomes more immediate, natural, and engaging. It means:</p>
      <ul>
        <li>‚úÖ more autonomy,</li>
        <li>‚úÖ more inclusion,</li>
        <li>‚úÖ more opportunities for learning, entertainment, and social participation.</li>
      </ul>
      <p>Our goal is to build a more empathetic world, where hearing and deaf people can communicate without barriers. Technology speaks with hands. And finally, everyone can listen.</p>
    </section>
  </main>

  <footer>
    &copy; 2025 EchoSynch. All rights reserved.
  </footer>
</body>
</html>
